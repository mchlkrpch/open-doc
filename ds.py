import csv
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from bs4 import BeautifulSoup
import PyPDF2
import pandas as pd

class OpenDocQA:
    def __init__(self, csv_path: str, document_path: Optional[str] = None):
        """
        Initialize QA Dataset for RAG evaluation.
        
        Args:
            csv_path: Path to CSV file containing questions and answers
            document_path: Optional path to source document (HTML/PDF)
        """
        self.questions = []
        self.reference_answers = []
        self.document_text = ""
        
        self._load_qa_pairs(csv_path)
        
        if document_path:
            self.load_document(document_path)
    
    def _load_qa_pairs(self, csv_path: str) -> None:
        """
        Load questions and answers from CSV file.
        Expected CSV format: question,answer columns
        
        Args:
            csv_path: Path to CSV file
        """
        try:
            with open(csv_path, 'r', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                for row in reader:
                    self.questions.append(row['question'])
                    self.reference_answers.append(row['answer'])
        except Exception as e:
            raise ValueError(f"Error loading QA pairs from {csv_path}: {str(e)}")
    
    def load_document(self, document_path: str) -> None:
        """
        Load and extract text from document (HTML or PDF)
        
        Args:
            document_path: Path to document file
        """
        path = Path(document_path)
        if not path.exists():
            raise FileNotFoundError(f"Document not found: {document_path}")
        
        if path.suffix.lower() == '.html':
            self._load_html_document(path)
        elif path.suffix.lower() == '.pdf':
            self._load_pdf_document(path)
        else:
            raise ValueError("Unsupported document format. Only HTML and PDF are supported.")
    
    def _load_html_document(self, path: Path) -> None:
        """Extract all text from HTML document"""
        try:
            with open(path, 'r', encoding='utf-8') as file:
                soup = BeautifulSoup(file, 'html.parser')
                # Remove script and style elements
                for script in soup(["script", "style"]):
                    script.decompose()
                self.document_text = soup.get_text(separator=' ', strip=True)
                # Clean up excessive whitespace
                self.document_text = re.sub(r'\s+', ' ', self.document_text)
        except Exception as e:
            raise ValueError(f"Error processing HTML document: {str(e)}")
    
    def _load_pdf_document(self, path: Path) -> None:
        """Extract all text from PDF document"""
        try:
            with open(path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = []
                for page in reader.pages:
                    text.append(page.extract_text())
                self.document_text = '\n'.join(text)
                # Clean up excessive whitespace
                self.document_text = re.sub(r'\s+', ' ', self.document_text)
        except Exception as e:
            raise ValueError(f"Error processing PDF document: {str(e)}")
    
    def evaluate_rag(self, generated_answers: List[str], metrics: List[str] = ['exact_match', 'f1']) -> Dict[str, List[float]]:
        """
        Evaluate RAG performance against reference answers
        
        Args:
            generated_answers: List of answers generated by RAG system
            metrics: List of metrics to compute ('exact_match', 'f1', 'bleu', 'rouge')
            
        Returns:
            Dictionary with metric names as keys and lists of scores as values
        """
        if len(generated_answers) != len(self.reference_answers):
            raise ValueError("Number of generated answers must match number of reference answers")
        
        results = {metric: [] for metric in metrics}
        
        for gen_answer, ref_answer in zip(generated_answers, self.reference_answers):
            if 'exact_match' in metrics:
                results['exact_match'].append(self._exact_match(gen_answer, ref_answer))
            if 'f1' in metrics:
                results['f1'].append(self._f1_score(gen_answer, ref_answer))
            if 'bleu' in metrics:
                results['bleu'].append(self._bleu_score(gen_answer, ref_answer))
            if 'rouge' in metrics:
                results['rouge'].append(self._rouge_score(gen_answer, ref_answer))
        
        return results
    
    def _exact_match(self, gen_answer: str, ref_answer: str) -> float:
        """Compute exact match score (1 if answers match exactly, 0 otherwise)"""
        return float(gen_answer.strip().lower() == ref_answer.strip().lower())
    
    def _f1_score(self, gen_answer: str, ref_answer: str) -> float:
        """Compute F1 score between generated and reference answer"""
        gen_tokens = set(gen_answer.lower().split())
        ref_tokens = set(ref_answer.lower().split())
        
        if not gen_tokens or not ref_tokens:
            return 0.0
        
        common_tokens = gen_tokens & ref_tokens
        precision = len(common_tokens) / len(gen_tokens)
        recall = len(common_tokens) / len(ref_tokens)
        
        if (precision + recall) == 0:
            return 0.0
        
        return 2 * (precision * recall) / (precision + recall)
    
    def _bleu_score(self, gen_answer: str, ref_answer: str) -> float:
        """Compute BLEU score (simplified version)"""
        # Note: For production use, consider using nltk's bleu_score
        gen_words = gen_answer.lower().split()
        ref_words = ref_answer.lower().split()
        
        if not gen_words or not ref_words:
            return 0.0
        
        common_words = set(gen_words) & set(ref_words)
        return len(common_words) / len(ref_words)
    
    def _rouge_score(self, gen_answer: str, ref_answer: str) -> float:
        """Compute ROUGE-L score (simplified version)"""
        # Note: For production use, consider using rouge-score package
        gen_words = gen_answer.lower().split()
        ref_words = ref_answer.lower().split()
        
        if not gen_words or not ref_words:
            return 0.0
        
        # Find longest common subsequence
        lcs_length = self._lcs(gen_words, ref_words)
        recall = lcs_length / len(ref_words)
        precision = lcs_length / len(gen_words)
        
        if (precision + recall) == 0:
            return 0.0
        
        return 2 * (precision * recall) / (precision + recall)
    
    def _lcs(self, a: List[str], b: List[str]) -> int:
        """Compute length of longest common subsequence"""
        lengths = [[0] * (len(b)+1) for _ in range(len(a)+1)]
        for i, x in enumerate(a):
            for j, y in enumerate(b):
                if x == y:
                    lengths[i+1][j+1] = lengths[i][j] + 1
                else:
                    lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])
        return lengths[-1][-1]
    
    def to_dataframe(self) -> pd.DataFrame:
        """Convert QA pairs to pandas DataFrame"""
        return pd.DataFrame({
            'question': self.questions,
            'reference_answer': self.reference_answers
        })
    
    def __len__(self) -> int:
        """Number of QA pairs in the dataset"""
        return len(self.questions)
    
    def __getitem__(self, idx: int) -> Tuple[str, str]:
        """Get question and reference answer by index"""
        return self.questions[idx], self.reference_answers[idx]